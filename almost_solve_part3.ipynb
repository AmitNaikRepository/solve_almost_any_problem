{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "almost_solve_part3",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNH1WfczB5c5Vm8R8xppbEd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmitNaikRepository/solve_almost_any_problem/blob/main/almost_solve_part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stqe3Jyuvngc"
      },
      "source": [
        "#import all the liabraries that matters \r\n",
        "import pandas as pd \r\n",
        "import numpy as np \r\n",
        "import glob \r\n",
        "import joblib \r\n",
        "import os \r\n",
        "import gc \r\n",
        "from sklearn import preprocessing, metrics   \r\n",
        "from tensorflow.keras.Layers import Embedding\r\n",
        "from tensorflow.keras import Layers \r\n",
        "from tensorflow.keras import Optimizers \r\n",
        "from tensorflow.keras import callbacks \r\n",
        "from tensorflow.keras.model import model,load_model \r\n",
        "from tensorflow.keras import utils \r\n",
        "from tensorflow.keras import backend as k \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjaDIe6YBZfR"
      },
      "source": [
        "def create_model(df,catcol):\r\n",
        "  # now we have cat colms suppose we have lot of cat data that have lot of feature this function will give us embedding layers \r\n",
        "\r\n",
        "  #data = df data \r\n",
        "  #catcol= list of categorical column \r\n",
        "  #return the complie model \r\n",
        "\r\n",
        "\r\n",
        "  #initialize the the input embedding \r\n",
        "  inputs=[]\r\n",
        "\r\n",
        "  #output embedding in the list \r\n",
        "  outputs=[]\r\n",
        "\r\n",
        "\r\n",
        "  for c in catcols:\r\n",
        "    #wel find number of unique value in cat col \r\n",
        "    num_unique_value=int(data[c].nunique())\r\n",
        "\r\n",
        "    embed_dim=int(min(np.ceil((num_unique_value)/2),50))\r\n",
        "\r\n",
        "    #simple keras layers with input size 1 \r\n",
        "\r\n",
        "    input= layers.Input(int)\r\n",
        "\r\n",
        "    #add embedding layers in the input layers \r\n",
        "\r\n",
        "    #embedding size is always 1 of more than unique value in the input \r\n",
        "\r\n",
        "    out =Embedding(num_unique_value + 1 ,embed_dim,name=c)(input)\r\n",
        "\r\n",
        "    #1-d spacial dropout is the standard for embedding layers\r\n",
        "    # you can use it nlp task for that \r\n",
        "\r\n",
        "    out=layers.SpacialDropout1d(0.3)(out)\r\n",
        "\r\n",
        "\r\n",
        "    #now reshape the input dimention with embedding \r\n",
        "    #now this is output layer fro the perticular feature-set\r\n",
        "    \r\n",
        "    out=layers.Reshape(target_shape=(embed_dim,))(out)\r\n",
        "\r\n",
        "    input.append(input)\r\n",
        "\r\n",
        "    output.append(output)\r\n",
        "\r\n",
        "    #Now concat to all the output layers\r\n",
        "    x=layers.Concatenate()(outputs)\r\n",
        "\r\n",
        "    x.layers.batchNormalization()(x)\r\n",
        "\r\n",
        "    #now we have bunch of layers with dropout \r\n",
        "    #start with 1 or two layers only \r\n",
        "\r\n",
        "    x=layers.Dense(300,activation='relu')(x)\r\n",
        "    x=layers.Dense(0.3)(X)\r\n",
        "    x=layers.batchNormalization()(X)\r\n",
        "\r\n",
        "    x=layers.Dense(300,activation='relu')(x)\r\n",
        "    x=layers.Dense(0.3)(x)\r\n",
        "    x=layers.batchNormalization()(x)\r\n",
        "\r\n",
        "    #now using softmaz when you have 2 categories but if you have more target variable you can use other \r\n",
        "    y=Layers.Dense(2,activation='softmax')(x)\r\n",
        "\r\n",
        "\r\n",
        "    model=Model(inputs=inputs,outputs=y)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    #now compile the model and use the binary crosentropy \r\n",
        "\r\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam')\r\n",
        "\r\n",
        "    return model \r\n",
        "\r\n",
        "def run(fold):\r\n",
        "  df =pd.read_csv('../input/cat_train.csv')\r\n",
        "\r\n",
        "  #all columns with the except id , target and kfolds \r\n",
        "  features=[\r\n",
        "            f for f in df.columns if f not in ('id','target','kfold')\r\n",
        "\r\n",
        "  ]\r\n",
        "  for col in features:\r\n",
        "    df.col[:,col]=df[col].astype(str).fillna('None')\r\n",
        "  for feat in features:\r\n",
        "    lbl_enc=preprocessing.LabelEncoder()\r\n",
        "    df.loc[:,feat]=lbl_enc.fit_transform(df[feat].values)\r\n",
        "\r\n",
        "    df_train=\r\n",
        "\r\n",
        "    df_valid=\r\n",
        "\r\n",
        "    model=\r\n",
        "\r\n",
        "    x_train=\r\n",
        "\r\n",
        "    x_valid=\r\n",
        "\r\n",
        "    #fetch the target column \r\n",
        "    y_train=\r\n",
        "\r\n",
        "    y_valid=\r\n",
        "\r\n",
        "    #convert the target column to categories \r\n",
        "    #this is binarization \r\n",
        "\r\n",
        "    ytraincat=\r\n",
        "    yvalid_cat=\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeZ4jTb5BevR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}